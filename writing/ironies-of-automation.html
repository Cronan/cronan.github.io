<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Bainbridge's Ironies of Automation, forty years on — Ivan Cronyn</title>
  <meta name="description" content="Lisanne Bainbridge's 1983 paper on the ironies of automation maps precisely onto how we're integrating AI into modern software systems.">
  <link rel="stylesheet" href="/styles.css">
</head>
<body>

  <nav>
    <a href="/">Home</a>
    <a href="/writing/" class="active">Writing</a>
    <a href="/about/">About</a>
  </nav>

  <article>
    <header>
      <h1>Bainbridge's Ironies of Automation, forty years on</h1>
      <p class="date">2025</p>
    </header>

    <hr>

    <p>
      I read a great Hacker News post that started a deep dive. Lisanne
      Bainbridge's <em>Ironies of Automation</em> was written in 1983, and
      it's uncanny how precisely it maps onto the way we're integrating AI
      into modern software systems.
    </p>

    <p>
      Her core insight wasn't "humans make mistakes" — it was that automation
      tends to remove the very work that keeps humans competent, then hands
      responsibility back to them at the worst possible moment.
    </p>

    <p>
      That's starting to show up clearly with AI.
    </p>

    <p>
      Most "human-in-the-loop" designs I see today are structurally flawed.
      We let models do the reasoning, the exploration, the pattern-finding —
      and then ask a human to approve or override the output without having
      participated in the process that produced it. At that point, the human
      isn't providing judgment; they're absorbing liability.
    </p>

    <p>
      What's interesting is that tools like Claude Code can go either way here.
    </p>

    <p>
      Used naively, they accelerate deskilling: fewer reps, less context,
      more opaque decisions. Used deliberately — with reusable skills,
      explicit structure, tests, constraints, and review loops — they can
      do the opposite: preserve the human mental model while stripping
      away boilerplate.
    </p>

    <p>
      That distinction feels like the real leadership challenge with AI:
    </p>

    <ul>
      <li>Not how much we automate</li>
      <li>But which cognitive work we remove, and which we deliberately keep humans engaged in</li>
    </ul>

    <p>
      If an AI system is too complex for a human to understand during normal
      operation, it's unrealistic — and unsafe — to expect understanding to
      suddenly appear during failure.
    </p>

    <p>
      The future isn't "humans vs AI" or even "humans supervising AI."
      It's systems that are designed so humans stay epistemically in the loop,
      not just formally on the hook.
    </p>

  </article>

  <footer>
    <a href="/writing/">← All writing</a> · <a href="/">Ivan Cronyn</a>
  </footer>

</body>
</html>
