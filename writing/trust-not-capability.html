<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Trust, not capability — Ivan Cronyn</title>
  <meta name="description" content="Most AI discussions miss the real constraint: trust, not capability. The models are already good enough. The bottleneck is confidence.">
  <link rel="stylesheet" href="/styles.css">
</head>
<body>

  <nav>
    <a href="/">Home</a>
    <a href="/writing/" class="active">Writing</a>
    <a href="/about/">About</a>
  </nav>

  <article>
    <header>
      <h1>Trust, not capability: the real bottleneck in AI-assisted engineering</h1>
      <p class="date">2025</p>
    </header>

    <hr>

    <p>
      Most AI discussions miss the real constraint: trust, not capability.
    </p>

    <p>
      The models are already good enough to write useful code. The bottleneck
      isn't accuracy — it's confidence. If senior engineers don't trust the
      changes, velocity collapses back to baseline.
    </p>

    <p>
      The teams making progress aren't arguing about which model is best.
      They're investing in:
    </p>

    <ul>
      <li>Making changes observable</li>
      <li>Making failures reversible</li>
      <li>Letting automation enforce rules humans shouldn't have to remember</li>
    </ul>

    <p>
      That's not "AI safety" as a moral stance — it's basic software
      engineering applied to new tooling.
    </p>

    <p>
      AI is only a force multiplier if the underlying system is designed to
      absorb mistakes cheaply. Everything else is a demo.
    </p>

    <p>
      Time to stop vibe-coding and start building value.
    </p>

  </article>

  <footer>
    <a href="/writing/">← All writing</a> · <a href="/">Ivan Cronyn</a>
  </footer>

</body>
</html>
