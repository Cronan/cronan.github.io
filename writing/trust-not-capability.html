<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Trust, not capability - Ivan Cronyn</title>
  <meta name="description" content="Most AI discussions miss the real constraint: trust, not capability. The models are already good enough. The bottleneck is confidence.">
  <link rel="stylesheet" href="/styles.css?v=3">
</head>
<body>

  <nav>
    <a href="/">Home</a>
    <a href="/writing/" class="active">Writing</a>
    <a href="/reading/">Reading</a>
    <a href="/about/">About</a>
  </nav>

  <article>
    <header>
      <h1>Trust, not capability: the real bottleneck in AI-assisted engineering</h1>
      <p class="date">September 2025</p>
    </header>

    <hr>

    <p>
      Most AI discussions miss the real constraint: trust, not capability.
    </p>

    <p>
      The models are already good enough to write useful code. GPT-4, Claude,
      Gemini - they can all produce working functions, reasonable tests, decent
      refactors. The bottleneck isn't accuracy. It's confidence. If senior
      engineers don't trust the changes, velocity collapses back to baseline.
    </p>

    <p>
      I keep seeing the same sequence. The pattern has become familiar
      enough that I can usually call the next step before it happens. Someone introduces an AI coding tool. Output goes up.
      Then code review starts taking longer, because reviewers can't tell
      whether the person submitting the PR actually understands the code they're
      submitting. PRs get bigger. Context gets thinner. Senior engineers start
      pushing back - not because they're resistant to change, but because
      they've been burned before by code nobody fully understood.
    </p>

    <p>
      The teams making progress aren't arguing about which model is best.
      They're investing in:
    </p>

    <ul>
      <li>Making changes observable</li>
      <li>Making failures reversible</li>
      <li>Letting automation enforce rules humans shouldn't have to remember</li>
    </ul>

    <p>
      That's not "AI safety" as a moral stance. It's basic software
      engineering applied to new tooling.
    </p>

    <h2>What observable means in practice</h2>

    <p>
      When I say observable, I mean the reviewer can see what changed, why it
      changed, and what the expected behaviour is - without having to reverse-engineer
      intent from a diff. That means small PRs with clear descriptions.
      It means tests that document intent, not just coverage. It means commit
      messages that explain the reasoning, not just the action.
    </p>

    <p>
      None of this is new. Good teams have done this for years. But AI-assisted
      coding makes it more important, because the person submitting the code may
      not have written every line themselves. The reviewer needs to trust the
      process, not just the author.
    </p>

    <h2>What reversible means in practice</h2>

    <p>
      If a bad change gets through, how expensive is it? If the answer is
      "we roll back the deploy and lose ten minutes," that's cheap. If the
      answer is "we corrupt production data and spend a week recovering,"
      that's not.
    </p>

    <p>
      Feature flags, canary deployments, database migrations that can be
      undone - these are the infrastructure that lets you absorb mistakes.
      AI makes more mistakes than a careful senior engineer. But it also
      moves faster. Whether that's a net win depends entirely on how cheaply
      you can recover from the mistakes.
    </p>

    <h2>What enforcement means in practice</h2>

    <p>
      Humans forget things. They forget to run the linter. They forget to
      update the changelog. They forget that module X has an implicit
      dependency on module Y. CI pipelines don't forget. Static analysis
      doesn't forget.
    </p>

    <p>
      The more you can encode your team's standards into automated checks,
      the less you need to rely on reviewers catching problems by eye. This
      matters especially with AI-generated code, because the failure modes
      are different. AI won't misspell a variable name, but it will happily
      introduce a subtle race condition and wrap it in perfectly formatted
      code.
    </p>

    <h2>The real question</h2>

    <p>
      AI is only a force multiplier if the underlying system is designed to
      absorb mistakes cheaply. Everything else is a demo.
    </p>

    <p>
      The question isn't "how good is the model?" It's "how good is
      your engineering system at catching and recovering from errors,
      regardless of their source?" If your CI pipeline catches real errors,
      your test coverage is honest, rollbacks are fast, and review
      processes are clear, AI tools will accelerate your team. If not,
      they'll accelerate your problems.
    </p>

    <h2>What this means for technology leaders</h2>

    <p>
      This framing changes where the investment goes. Most AI discussions
      focus on which tools to adopt, which models to use, how to train
      people on prompting. Those matter, but they're secondary. The primary
      investment should be in the trust infrastructure: the pipelines, tests,
      observability, and rollback mechanisms that let you move fast without
      breaking things.
    </p>

    <p>
      In financial services, "absorb mistakes cheaply" has a specific meaning.
      Can you roll back a deployment without affecting end-of-day processing?
      Can you detect a calculation error before it hits a client report? Can
      you explain to a regulator exactly what changed and when? If the answer
      to any of these is "not confidently," then you're not ready for AI to
      accelerate your engineering velocity. You're ready for it to accelerate
      your operational risk.
    </p>

    <p>
      Adoption speed doesn't predict who wins here. The advantage goes to
      firms that invested in engineering discipline before AI arrived, and
      now have somewhere solid to plug it in. The rest will adopt the same
      tools and wonder why the results feel worse.
    </p>

    <p>
      Before asking "which AI tools should we use?", ask "what happens when
      they get it wrong?" If you don't like the answer, that's your
      backlog.
    </p>

  </article>

  <footer>
    <a href="/writing/">← All writing</a> · <a href="/">Ivan Cronyn</a>
  </footer>

</body>
</html>
